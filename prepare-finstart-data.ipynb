{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and loading all data \n",
    "after the data is retrievded from yahoo, all data is stored in a S3 bucket in csv files, as part of the project I also converted the data into json format and stored it in the S3 bucket. The entire smoothness of process depends on the upload speed of your internet. In my case the upload speed is pretty slow, therefore it's possible to comment out the procedures in the main body. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "##!pip install pandas-datareader\n",
    "#!pip install matplotlib\n",
    "#!pip install beautifullsoup4\n",
    "#!pip install scikit-learn\n",
    "#!pip install sklearn\n",
    "\n",
    "#!pip install https://github.com/matplotlib/mpl_finance/archive/master.zip\n",
    "#!pip install git+https://github.com/pydata/pandas-datareader.git\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the packages once installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import boto3\n",
    "import configparser\n",
    "import psycopg2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the configurations and get the KEY and SECRET for AWS\n",
    "all configuration parameters can be stored in the dwh.cfg file and is based on a KEY = VALUE notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# procedures and functions to collect and store data\n",
    "\n",
    "## A function that writes a file into the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    write_to_s3\n",
    "    ___________________________________________________\n",
    "    description: it writes the files in an s3 provided bucket. \n",
    "    parameters: \n",
    "        filename: the full filename, including the subfolder where the file is located in your notebook\n",
    "        bucket: the name of the s3 bucket\n",
    "        key: the full subfolder and filname where you want the file to be copied\n",
    "    return: n/a\n",
    "\"\"\"\n",
    "def write_to_s3(filename, bucket, key):    \n",
    "    with open(filename, 'rb') as f:\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following procedure gets a list with all S&P companies\n",
    "The list is retrieved from wikipedia by using beautifulsoup4 where the data is stored in a serialized file and addionally copied into the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    pickle_sp500_companies\n",
    "    ___________________________________________________\n",
    "    \n",
    "    description: scrape the S&P500 companies table from wikipedia. Use pickle to serialize and save the table\n",
    "    parameters: n/a\n",
    "    return: A pickled representation of all S&P 500 companies\n",
    "\"\"\"\n",
    "def pickle_sp500_tickers():\n",
    "    sp500 = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(sp500.text, \"lxml\")\n",
    "    sp500_table = soup.find('table', {'class':'wikitable sortable'})\n",
    "    tickers = []\n",
    "    \n",
    "    for row in sp500_table.findAll('tr')[1:]:\n",
    "        symbol       = row.findAll('td')[0].text\n",
    "        company_name = row.findAll('td')[1].text\n",
    "        wiki         = 'https://en.wikipedia.org' + row.findAll('td')[1].a['href']\n",
    "        sector       = row.findAll('td')[3].text\n",
    "        subSector    = row.findAll('td')[4].text\n",
    "        \n",
    "        tickers.append([symbol.replace('\\n', '').replace('.','-'),company_name.replace('\\n', ''),wiki.replace('\\n', ''),sector.replace('\\n', ''),subSector.replace('\\n', '')])\n",
    "    \n",
    "    # write the tickers into a CSV file and upload to S3 for further processing\n",
    "    df = pd.DataFrame(tickers)\n",
    "    df.to_csv('stock_dfs/sp500_companies.csv',header=False,sep=\";\")\n",
    "    write_to_s3('stock_dfs/sp500_companies.csv','cda-dend-capstone','stock_dfs/sp500_companies.csv')\n",
    "    \n",
    "    \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers, f)\n",
    "        \n",
    "    \n",
    "    return tickers\n",
    "\n",
    "#pickle_sp500_tickers() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The S&P companies file is used to retrieve ticker info for each company\n",
    "The ticker data is stored in CSV and JSON files and uploaded to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    get_stock_data_from_yahoo_financials\n",
    "    ___________________________________________________\n",
    "    \n",
    "    description: This method will get a ticker for each company in the sp500tickers.picle file. \n",
    "                 For each ticker a CSV and JSON file is created. \n",
    "    parameters: reload_sp500 (True or false)\n",
    "        True: it will execute the function pickle_sp500_tickers once more\n",
    "        False: It's not required to retrieve each time all S&P 500 companies for this project\n",
    "    return: n/a\n",
    "\"\"\"\n",
    "def get_stock_data_from_yahoo_financials(reload_sp500):\n",
    "    if reload_sp500:\n",
    "        tickers = pickle_sp500_tickers()\n",
    "    else:\n",
    "        with open (\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    \n",
    "    # I prefer to recreate all content, at least for now. \n",
    "    if os.path.exists('stock_dfs/csv'):\n",
    "        shutil.rmtree('stock_dfs/csv')\n",
    "    os.makedirs('stock_dfs/csv')\n",
    "    \n",
    "    if os.path.exists('stock_dfs/json'):\n",
    "        shutil.rmtree('stock_dfs/json')\n",
    "    os.makedirs('stock_dfs/json')\n",
    "\n",
    "    # 2000 is a nice number to start with and collects lots of information\n",
    "    start = dt.datetime(2000,1,1)\n",
    "    end = dt.datetime.now()\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # build a dataframe with all ticker information from yahoo and set the date value as the index\n",
    "        df = web.DataReader(ticker[0], 'yahoo', start, end)\n",
    "        df.reset_index(inplace=True)\n",
    "        df.set_index('Date', inplace=True)\n",
    "        df.insert(0, 'company', ticker[0])\n",
    "            \n",
    "        # store each ticker into it's own csv file on a folder located together with the notebooks \n",
    "        df.to_csv('stock_dfs/csv/{}.csv'.format(ticker[0]))\n",
    "        \n",
    "        # convert the csv into json\n",
    "        with open('stock_dfs/csv/{}.csv'.format(ticker[0])) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            rows = list(reader)  \n",
    "        with open('stock_dfs/json/{}.json'.format(ticker[0]), 'w') as f:\n",
    "            json.dump(rows, f)\n",
    "        \n",
    "        # convert the csv into json\n",
    "        write_to_s3('stock_dfs/csv/{}.csv'.format(ticker[0]),'cda-dend-capstone','stock_dfs/csv/{}.csv'.format(ticker[0]))\n",
    "        write_to_s3('stock_dfs/json/{}.json'.format(ticker[0]),'cda-dend-capstone','stock_dfs/json/{}.json'.format(ticker[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEFORE YOU CAN CONTINUE \n",
    "## run the etl.py with the parameter equal to 'dim'\n",
    "from a terminal run: `python etl.py dim`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data is loaded in the dimensional tables, let's continue with the fact tables\n",
    "The fact tables are actually strategy indicators. I only implemented 3 strategies but it's possible tio implement many more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    load_sp500_sma30_data\n",
    "    ___________________________________________________\n",
    "    \n",
    "    description: A detailed description of the SMA 30 can be found in the 'test and plot dataset' notebook\n",
    "                 This method creates a CSV for each company in CSV format the S3 bucket\n",
    "    parameters: reload_sp500 (True or false)\n",
    "        True: it will execute the function pickle_sp500_tickers once more\n",
    "        False: It's not required to retrieve each time all S&P 500 companies for this project\n",
    "    return: n/a\n",
    "\"\"\"\n",
    "def load_sp500_sma30_data(reload_sp500):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    if reload_sp500:\n",
    "        tickers = pickle_sp500_tickers()\n",
    "    else:\n",
    "        with open (\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    \n",
    "    for ticker in tickers:    \n",
    "        df = pd.read_sql_query(\"select date, adj_close from public.sp500_tickers where company = '{}' order by date asc\".format(ticker[0]), conn, index_col='date')\n",
    "        df['30 SMA'] = pd.Series.rolling(df, 30).mean()\n",
    "        df.dropna(inplace=True)\n",
    "        #print(df.head())\n",
    "        df['company'] = ticker[0]\n",
    "        #print(df)\n",
    "        df.to_csv('stock_dfs/sma30/{}.csv'.format(ticker[0]))\n",
    "\n",
    "        write_to_s3('stock_dfs/sma30/{}.csv'.format(ticker[0]),'cda-dend-capstone','stock_dfs/sma30/{}.csv'.format(ticker[0]))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    load_sp500_bollinger_data\n",
    "    ___________________________________________________\n",
    "    \n",
    "    description: A detailed description of Bollinger can be found in the 'test and plot dataset' notebook\n",
    "                 This method creates a CSV for each company in CSV format the S3 bucket\n",
    "    parameters: reload_sp500 (True or false)\n",
    "        True: it will execute the function pickle_sp500_tickers once more\n",
    "        False: It's not required to retrieve each time all S&P 500 companies for this project\n",
    "    return: n/a\n",
    "\"\"\"\n",
    "def load_sp500_bollingerBand_data(reload_sp500):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    if reload_sp500:\n",
    "        tickers = pickle_sp500_tickers()\n",
    "    else:\n",
    "        with open (\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    \n",
    "    for ticker in tickers:    \n",
    "        df = pd.read_sql_query(\"select date, adj_close from public.sp500_tickers where company = '{}' order by date asc\".format(ticker[0]), conn, index_col='date')\n",
    "        \n",
    "        df['20_ma'] = pd.Series.rolling(df['adj_close'], 20).mean()\n",
    "        \n",
    "        df['20_sd'] = pd.Series.rolling(df['adj_close'], 20).std()\n",
    "        \n",
    "        df['Upper_Band'] = df['20_ma'] + (df['20_sd']*2)\n",
    "        df['Lower_Band'] = df['20_ma'] - (df['20_sd']*2)\n",
    "        df.dropna(inplace=True)\n",
    "        #print(df.head())\n",
    "        df['company'] = ticker[0]\n",
    "        #print(df)\n",
    "        df.to_csv('stock_dfs/bollinger/{}.csv'.format(ticker[0]))\n",
    "\n",
    "        write_to_s3('stock_dfs/bollinger/{}.csv'.format(ticker[0]),'cda-dend-capstone','stock_dfs/bollinger/{}.csv'.format(ticker[0]))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    load_sp500_MACD_data\n",
    "    ___________________________________________________\n",
    "    \n",
    "    description: A detailed description of the MACD can be found in the 'test and plot dataset' notebook\n",
    "                 This method creates a CSV for each company in CSV format the S3 bucket\n",
    "    parameters: reload_sp500 (True or false)\n",
    "        True: it will execute the function pickle_sp500_tickers once more\n",
    "        False: It's not required to retrieve each time all S&P 500 companies for this project\n",
    "    return: n/a\n",
    "\"\"\"\n",
    "def load_sp500_MACD_data(reload_sp500):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    if reload_sp500:\n",
    "        tickers = pickle_sp500_tickers()\n",
    "    else:\n",
    "        with open (\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "            \n",
    "    # first check if the macd folder exist, delete the content if it does (at least for now)        \n",
    "    if os.path.exists('stock_dfs/macd'):\n",
    "        shutil.rmtree('stock_dfs/macd')\n",
    "    os.makedirs('stock_dfs/macd')\n",
    "        \n",
    "    for ticker in tickers:    \n",
    "        df = pd.read_sql_query(\"select date, adj_close from public.sp500_tickers where company = '{}' order by date asc\".format(ticker[0]), conn, index_col='date')\n",
    "        \n",
    "        df['30_mavg'] = pd.Series.rolling(df['adj_close'], 30).mean()\n",
    "        df['26_ema'] = pd.Series.ewm(df['adj_close'], 26).mean()\n",
    "        df['12_ema'] = pd.Series.ewm(df['adj_close'], 12).mean()\n",
    "        df['MACD'] = (df['12_ema'] - df['26_ema'])\n",
    "        df['Signal'] = pd.Series.ewm(df['MACD'], 9).mean()\n",
    "        df['Crossover'] = df['MACD'] - df['Signal']\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        #print(df.head())\n",
    "        df['company'] = ticker[0]\n",
    "        #print(df)\n",
    "        \n",
    "        df.to_csv('stock_dfs/macd/{}.csv'.format(ticker[0]))\n",
    "\n",
    "        write_to_s3('stock_dfs/macd/{}.csv'.format(ticker[0]),'cda-dend-capstone','stock_dfs/macd/{}.csv'.format(ticker[0]))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "        main procedure\n",
    "        \n",
    "        Parameter:\n",
    "            - nil\n",
    "\n",
    "        Return:\n",
    "            - nil\n",
    "    \"\"\"\n",
    "    # parameter: reload_sp500\n",
    "    #      True: create a sp500_companies.csv file, \n",
    "    #      False: use the existing sp500_companies.csv file\n",
    "    #get_stock_data_from_yahoo_financials(False)\n",
    "    #load_sp500_sma30_data(False)\n",
    "    #load_sp500_bollingerBand_data(False)\n",
    "    #load_sp500_macd_data(False)\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEFORE YOU TEST AND PLOT THE DATA\n",
    "## run the etl.py with the parameter equal to 'fact' to load the fact tables in the database\n",
    "\n",
    "from a terminal run: `python etl.py fact`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
